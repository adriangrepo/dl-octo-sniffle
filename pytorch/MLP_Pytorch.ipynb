{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3VsmcZU4O8I"
   },
   "source": [
    "# Multi-layer Perceptron (MLP)\n",
    "\n",
    "Implementation of Rosenblatt's Multi-layer Peceptron in Pytorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CHadwpvmREvE"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from thop import profile\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_ID=0\n",
    "device = torch.device(f\"cuda:{GPU_ID}\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixed parameters\n",
    "IMAGE_SIZE = 28  #MNIST image size\n",
    "OUT_CLASSES = 10 #MNIST has 10 classes\n",
    "INPUT_DIM = IMAGE_SIZE*IMAGE_SIZE\n",
    "#train paparms\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "#Misc. params\n",
    "SEED = 42\n",
    "DATA_DIR = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "GPvVD4VLREva"
   },
   "outputs": [],
   "source": [
    "def generate_data():\n",
    "    train_data = datasets.MNIST(root=DATA_DIR, train=True, \n",
    "                                download=True, transform=transforms.ToTensor())\n",
    "    test_data = datasets.MNIST(root=DATA_DIR, train=False, \n",
    "                               download=True, transform=transforms.ToTensor())\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "sxEwZKjWROlm",
    "outputId": "1e4fbc62-6c01-496b-ab98-cf04f2b7a4e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f00595962d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "im = train_data[1][0]\n",
    "plt.imshow(im.squeeze(0), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2QI6JckYREve"
   },
   "outputs": [],
   "source": [
    "#could experiment with different train/validation ratios\n",
    "TRAIN_VAL_RATIO = 0.9\n",
    "\n",
    "n_train = int(len(train_data) * TRAIN_VAL_RATIO)\n",
    "n_valid = len(train_data) - n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6yVJvPZ-RVvI"
   },
   "outputs": [],
   "source": [
    "train_data, valid_data = random_split(train_data, [n_train, n_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "M5LZxzXGcsNk"
   },
   "outputs": [],
   "source": [
    "valid_data = copy.deepcopy(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "sGdKSORpREvr"
   },
   "outputs": [],
   "source": [
    "train_iterator = DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE)\n",
    "valid_iterator = DataLoader(valid_data, batch_size=BATCH_SIZE)\n",
    "test_iterator = DataLoader(test_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Ew0aobVREv0"
   },
   "source": [
    "Model size analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(net):\n",
    "    total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net_device(net):\n",
    "    #after mcunet\n",
    "    return net.parameters().__next__().device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MACs are multiply-accumulate operations](https://openaccess.thecvf.com/content/CVPR2022W/NAS/papers/Liu_Network_Amplification_With_Efficient_MACs_Allocation_CVPRW_2022_paper.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "gik6Q5ZB4O9q"
   },
   "outputs": [],
   "source": [
    "def count_net_flops(model, data_shape):\n",
    "    #after mcunet\n",
    "    model = copy.deepcopy(model)\n",
    "    total_macs, _ = profile(\n",
    "        model,\n",
    "        inputs=(torch.randn(*data_shape).to(get_net_device(model)),),\n",
    "        verbose=False,\n",
    "    )\n",
    "    del model\n",
    "    return total_macs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define NN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "lAqzcW9XREvu"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_in, hidden_out, out_classes):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(input_dim, hidden_in),   #input layer\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(hidden_in, hidden_out),  #hidden layer\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(hidden_out, out_classes) #output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = self.layers(x)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_2_Hidden(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_1_in, hidden_1_out, hidden_2_out, out_classes):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(input_dim, hidden_1_in),            #input layer\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(hidden_1_in, hidden_1_out),         #hidden layer 1\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(hidden_1_out, hidden_2_out),        #hidden layer 2\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(hidden_2_out, out_classes)                   #output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = self.layers(x)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "OO2BL-qBREwD"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "VkttGrmkREwP"
   },
   "outputs": [],
   "source": [
    "def calc_accuracy(y_pred, y):\n",
    "    top_pred = y_pred.argmax(1, keepdim=True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "0kCtKBMfREwS"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, device):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for (x, y) in iterator:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        acc = calc_accuracy(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "3XL4sRI8REwV"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, device):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (x, y) in iterator:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            acc = calc_accuracy(y_pred, y)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "id": "p-gtfzafREwc",
    "outputId": "a1314661-c18c-4de6-d6f5-89541165199e"
   },
   "outputs": [],
   "source": [
    "def train_val_workflow(model, model_save_name):\n",
    "    best_valid_loss = float('inf') #unbound upper value\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion, device)\n",
    "        valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, device)\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), model_save_name)\n",
    "        print(f'Epoch: {epoch}, Train Loss: {train_loss:.2f}, Val. Loss: {valid_loss:.2f}, Train Acc: {train_acc*100:.2f}%, Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP with 1 hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test these parameters\n",
    "hidden_in=128\n",
    "hidden_out=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(INPUT_DIM, hidden_in, hidden_out, OUT_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "eC2P8WJuREv_"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 104938, MACS: 6705152.0\n"
     ]
    }
   ],
   "source": [
    "#MNIST has only 1 channel\n",
    "data_shape=(BATCH_SIZE, 1, IMAGE_SIZE, IMAGE_SIZE)\n",
    "print(f'Parameters: {count_parameters(model)}, MACS: {count_net_flops(model, data_shape)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 0.38, Val. Loss: 0.20, Train Acc: 89.49%, Val. Acc: 94.17%\n",
      "Epoch: 1, Train Loss: 0.15, Val. Loss: 0.14, Train Acc: 95.59%, Val. Acc: 95.94%\n",
      "Epoch: 2, Train Loss: 0.11, Val. Loss: 0.13, Train Acc: 96.87%, Val. Acc: 95.90%\n",
      "Epoch: 3, Train Loss: 0.08, Val. Loss: 0.11, Train Acc: 97.52%, Val. Acc: 96.48%\n",
      "Epoch: 4, Train Loss: 0.06, Val. Loss: 0.10, Train Acc: 97.97%, Val. Acc: 97.02%\n",
      "Epoch: 5, Train Loss: 0.05, Val. Loss: 0.09, Train Acc: 98.44%, Val. Acc: 97.25%\n",
      "Epoch: 6, Train Loss: 0.04, Val. Loss: 0.10, Train Acc: 98.70%, Val. Acc: 97.14%\n",
      "Epoch: 7, Train Loss: 0.04, Val. Loss: 0.10, Train Acc: 98.86%, Val. Acc: 96.92%\n",
      "Epoch: 8, Train Loss: 0.03, Val. Loss: 0.11, Train Acc: 99.07%, Val. Acc: 96.99%\n",
      "Epoch: 9, Train Loss: 0.02, Val. Loss: 0.10, Train Acc: 99.34%, Val. Acc: 97.37%\n"
     ]
    }
   ],
   "source": [
    "train_val_workflow(model, model_save_name='mlp-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.086, Test Acc: 97.45%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('mlp-model.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion, device)\n",
    "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP with 2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test these parameters\n",
    "batch_size = 64\n",
    "hidden_1_in = 128\n",
    "hidden_1_out = 64\n",
    "hidden_2_out = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP_2_Hidden(INPUT_DIM, hidden_1_in, hidden_1_out, hidden_2_out, OUT_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 0.40, Val. Loss: 0.20, Train Acc: 88.37%, Val. Acc: 94.08%\n",
      "Epoch: 1, Train Loss: 0.15, Val. Loss: 0.15, Train Acc: 95.40%, Val. Acc: 95.68%\n",
      "Epoch: 2, Train Loss: 0.11, Val. Loss: 0.13, Train Acc: 96.79%, Val. Acc: 96.14%\n",
      "Epoch: 3, Train Loss: 0.08, Val. Loss: 0.11, Train Acc: 97.43%, Val. Acc: 96.62%\n",
      "Epoch: 4, Train Loss: 0.07, Val. Loss: 0.11, Train Acc: 97.90%, Val. Acc: 96.77%\n",
      "Epoch: 5, Train Loss: 0.05, Val. Loss: 0.11, Train Acc: 98.33%, Val. Acc: 96.97%\n",
      "Epoch: 6, Train Loss: 0.05, Val. Loss: 0.11, Train Acc: 98.52%, Val. Acc: 96.76%\n",
      "Epoch: 7, Train Loss: 0.04, Val. Loss: 0.10, Train Acc: 98.79%, Val. Acc: 97.12%\n",
      "Epoch: 8, Train Loss: 0.03, Val. Loss: 0.13, Train Acc: 99.02%, Val. Acc: 96.52%\n",
      "Epoch: 9, Train Loss: 0.03, Val. Loss: 0.11, Train Acc: 99.09%, Val. Acc: 97.02%\n"
     ]
    }
   ],
   "source": [
    "train_val_workflow(model, model_save_name='mlp-2h-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.095, Test Acc: 97.24%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('mlp-2h-model.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion, device)\n",
    "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "1 - Multilayer Perceptron.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
